Segment Anything Model (SAM) and its Application in Medical Image Segmentation of MRI Images
1. Introduction: The Segment Anything Model Ecosystem

The field of computer vision has witnessed a transformative innovation with the introduction of the Segment Anything Model (SAM) by Meta AI, a development poised to revolutionize image segmentation. Conventional segmentation methods often necessitate extensive training on meticulously labeled datasets specific to the objects of interest, presenting considerable challenges in terms of scalability and adaptability. SAM distinguishes itself by employing zero-shot learning, enabling it to generalize across diverse domains without the need for extensive retraining. This capability offers unprecedented flexibility and efficiency in identifying a wide array of objects within imagery. At its core, SAM possesses a remarkable ability to segment virtually any object in an image, a capability fueled by a comprehensive dataset known as Segment Anything 1-Billion mask dataset (SA-1B). With over 11 million images and 1 billion masks, SAM exhibits robustness in delineating object boundaries and discerning between various entities, transcending limitations inherent in domain-specific training. This foundational model has been integrated into platforms such as ArcGIS, underscoring its broad applicability. The introduction of SAM marked a significant departure from traditional segmentation approaches, offering unmatched flexibility, accuracy, and efficiency in extracting insights from images through zero-shot learning and extensive training data.   

Building upon the success of the original SAM, Meta AI released Segment Anything Model 2 (SAM2) in July 2024, extending the model's capabilities to include video segmentation while also improving its performance on image segmentation tasks. SAM, released in April 2023, primarily focused on image-based tasks. SAM2 represents a significant advancement by unifying image and video segmentation within a single model architecture. This evolution reflects a growing need to process more complex and dynamic visual data, a crucial aspect in various fields, including medical imaging where temporal sequences and 3D volumes are common.   

In the pursuit of specialized applications, Medical SAM2 has emerged as an adaptation of SAM2 tailored for the healthcare domain. This model focuses specifically on medical image segmentation, addressing the unique challenges presented by medical imagery, which often differs significantly from natural images in contrast, texture, and the shapes of objects of interest. The ultimate goal of this report is to explore the application of Medical SAM2 to Magnetic Resonance Imaging (MRI) images, a modality critical for medical diagnostics and research, highlighting the potential of these advanced segmentation models in advancing healthcare. The rapid progression from the initial SAM to specialized versions like Medical SAM2 demonstrates the profound impact of foundation models in computer vision and their capacity to address domain-specific challenges effectively. The inclusion of video processing in SAM2, a capability absent in the original SAM, indicates a clear trend towards handling more intricate and dynamic visual information, a development with direct relevance to medical imaging which frequently involves sequential data or volumetric scans.   

2. Segment Anything Model (SAM): Foundations

2.1 General Summary and Key Capabilities

The Segment Anything Model (SAM) is an innovative image segmentation model developed by Meta AI, designed to identify the precise location of either specific objects or all objects within an image. A key feature of SAM is its capacity for zero-shot transfer, enabling it to adapt to new image distributions and tasks without requiring prior knowledge or additional training. The development of SAM is part of the broader Segment Anything initiative, which introduced a novel model, task, and dataset specifically for image segmentation. SAM leverages prompt engineering as a primary mechanism for adapting to a diverse range of downstream segmentation problems, allowing users to guide the model using various types of prompts. These prompts can include foreground and background points, bounding boxes drawn around objects, and even rough masks provided by the user. While the original research paper explored the use of text prompts, this capability was not included in the initial public release of the model. SAM is engineered to produce a valid segmentation mask even when the provided prompt is ambiguous and could potentially refer to multiple objects within the image. This design philosophy, emphasizing promptability and zero-shot transfer, marks a significant shift from traditional image segmentation models that typically require extensive training tailored to specific tasks. The ability to segment based on flexible prompts and generalize to unseen data distributions makes SAM a versatile tool for a wide array of applications.   

2.2 Network Architecture

The architecture of SAM is composed of three primary components that work in concert to perform image segmentation: an image encoder, a prompt encoder, and a lightweight mask decoder.   

2.2.1 Image Encoder: The image encoder in SAM is based on a Vision Transformer (ViT) architecture, specifically a Masked Auto-Encoder (MAE) that has been adapted to process high-resolution images efficiently. This encoder is applied once per image to generate a comprehensive, one-time image embedding that captures high-level features present in the input. The ViT processes images by dividing them into smaller patches and then applies a series of transformer layers to capture both the spatial arrangement and the semantic meaning of the image content. The image encoder is available in three different sizes, each with a varying number of parameters, which impacts both the speed of inference and the overall performance of the model. These variants are ViT-B, containing 91 million parameters; ViT-L, with 308 million parameters; and ViT-H, the largest with 636 million parameters. While ViT-H generally offers the best segmentation accuracy, it is also the slowest among the three, whereas ViT-B provides the fastest inference but with a potential trade-off in accuracy. The ViT-H image encoder specifically has 632 million parameters.   

2.2.2 Prompt Encoder: The prompt encoder is designed to take various forms of user input, or prompts, and embed them into a feature space that is aligned with the image features extracted by the image encoder. This alignment is crucial for the model to understand which objects the user is interested in segmenting. The prompt encoder handles sparse prompts, such as points indicating foreground or background, and bounding boxes that define a rectangular region of interest. For these sparse prompts, the encoder uses positional embeddings, which encode the spatial location of the prompt, and these are summed with learned embeddings that are specific to the type of prompt (e.g., a point or a box). The prompt encoder also processes dense prompts, which are in the form of masks provided by the user. These mask prompts are embedded using convolutional neural networks (CNNs) and are combined element-wise with the image embeddings. The prompt encoder, along with the mask decoder, constitutes a relatively small portion of the overall model size, with a total of 4 million parameters between them.   

2.2.3 Mask Decoder: The mask decoder is a lightweight component of SAM that is based on a transformer architecture. Its role is to take the embeddings from both the image encoder and the prompt encoder and predict the segmentation masks for the objects indicated by the prompt. The mask decoder employs a Two-Way Transformer to effectively integrate the features coming from these two encoding pathways. Additionally, it includes an IoU (Intersection over Union) head, which is responsible for predicting a quality score for each of the segmentation masks that the decoder outputs. The decoder uses self-attention mechanisms to process the prompt embeddings and cross-attention mechanisms to incorporate information from the image embeddings, allowing it to refine the understanding of the object to be segmented. To handle potential ambiguities that can arise from a single prompt (e.g., a click on an object that could refer to the object itself or a part of it), SAM is designed to predict multiple valid segmentation masks for a given prompt. For each of these predicted masks, the model also trains a separate Multi-Layer Perceptron (MLP) to predict the IoU score, providing a measure of confidence in each mask's quality.   

The architectural design of SAM, which separates the computationally intensive image encoding into a one-time process and keeps the prompt encoding and mask decoding lightweight, is a crucial aspect that enables efficient prompting and real-time interaction after the initial image embedding is generated. Pre-computing the image embedding, which takes approximately 0.15 seconds on an NVIDIA A100 GPU, allows the model to rapidly generate masks for different prompts on the same image, with the prompt encoding and mask decoding taking only about 50 milliseconds on a CPU in a browser. This speed is essential for applications that require interactive segmentation. Furthermore, the use of Vision Transformers, particularly the MAE adapted for high-resolution images, for the image encoder is a key factor in SAM's ability to achieve accurate segmentation. ViTs are known for their capability to capture long-range dependencies and understand semantic information within images, which is vital for effectively segmenting objects of various types and sizes. The image encoder extracts high-level features from the input image, which are then upsampled to a higher resolution before being used by the mask decoder.   

2.3 Training Process

2.3.1 Dataset: SAM was trained using the Segment Anything 1-Billion mask dataset (SA-1B), which is the largest image segmentation dataset created to date. This massive dataset contains over 1 billion high-quality segmentation masks spread across approximately 11 million licensed and privacy-respecting images. The scale of SA-1B is remarkable, being 400 times larger than the next biggest mask dataset, and the image dataset itself is 6 times larger than OpenImages V5. The images included in SA-1B are diverse, feature high resolution (averaging 1500x2250 pixels), and are geographically representative, having been sourced from a large, reputable photo company. The masks within the dataset were annotated using a sophisticated model-in-the-loop "data engine," which involved three distinct stages. The first stage was assisted-manual annotation, where human annotators used an early version of SAM to generate initial masks and then refined these masks using interactive tools. The second stage aimed to increase the diversity of the masks by prompting human annotators to specifically identify and segment objects that the model had missed in its automatic predictions. Finally, the third stage involved a fully automatic process where the trained SAM model was prompted with a dense grid of points across each image, generating a large number of high-quality masks. This process resulted in an average of around 100 masks per image in the SA-1B dataset. The dataset encompasses a wide variety of objects, ranging from large structures like buildings to very detailed elements, ensuring a comprehensive training ground for the model.   

2.3.2 Training Methodology and Objectives: The training of SAM was centered around the promptable segmentation task, with the primary objective being to generate a valid and accurate segmentation mask for any given prompt, even in situations where the prompt might be ambiguous. The model's training was conducted progressively, in parallel with the ongoing development and expansion of the SA-1B dataset. The training process involved feeding the model with various types of prompts, including points, bounding boxes, and masks, and learning to predict the corresponding segmentation masks. This approach allowed the model to develop a generalized understanding of what constitutes an object, rather than learning to segment only specific categories. The design of the training task was inspired by the next token prediction task used in natural language processing. During training, the loss function used for the predicted masks was a combination of focal loss and dice loss, calculated with respect to the best performing mask among the three masks that the model was designed to predict. For the IoU scores that the model also predicts for each mask, the mean square error loss was used during training.   

2.3.3 Key Hyperparameters: While the specific hyperparameters used for the initial training of SAM on the SA-1B dataset are not explicitly detailed in the provided research snippets, information from studies that have fine-tuned SAM and SAM2 on downstream tasks offers insights into the types of hyperparameters that are critical for model adaptation. These include the learning rate, which controls the step size during optimization; the batch size, determining the number of samples processed in each training iteration; the choice of optimizer, with options like Adam, AdamW, and Lion being commonly used; the number of training epochs, indicating how many times the entire dataset is passed through the model; and the learning rate scheduler, such as StepLR, which adjusts the learning rate during training to help with convergence. Additionally, for optimal usage of SAM in applications like ArcGIS, parameters such as cell size and batch size can be adjusted to influence the model's performance. It is also worth noting the Sharpness Aware Minimization (SAM) optimizer, which aims to minimize both the loss and the sharpness of the loss landscape, and its associated hyperparameters like rho (controlling the neighborhood size) and interval (the frequency of applying the SAM algorithm), as this optimizer might have been employed during SAM's training to improve generalization.   

2.3.4 Approximate Training Time and Computational Resources: The original paper introducing SAM reported that the model was trained on a significant amount of computational resources, specifically 256 A100 GPUs for a duration of 68 hours. This training involved running for 2 epochs over the 11 million images contained in the SA-1B dataset. This scale of training underscores the vast computational power required to train a foundation model of this size and capability.   

The sheer scale and diversity of the SA-1B dataset were fundamental to SAM's remarkable zero-shot performance and its ability to generalize effectively to objects and images that it had never encountered during its training. By being trained on such a massive collection of segmentation masks covering a wide range of visual data, SAM developed a broad understanding of object boundaries and visual patterns across numerous domains. This enabled it to adapt to new segmentation tasks without the need for task-specific fine-tuning. Furthermore, the iterative data engine approach used to construct the SA-1B dataset, where the model itself was instrumental in generating additional training data, represents a highly sophisticated strategy for creating a large and diverse segmentation resource. This method not only allowed for efficient scaling of the dataset but also potentially facilitated the discovery and annotation of a more comprehensive range of objects and segmentation challenges than would have been feasible with purely manual annotation techniques.   

2.4 Evaluation Metrics and Benchmarks

The performance of SAM was evaluated across a diverse set of tasks and datasets to thoroughly assess its capabilities. These evaluations included zero-shot tasks, where SAM was applied to new datasets without any additional training. For instance, SAM was tested on single point mask evaluation across 23 publicly available datasets, where it often outperformed the strong baseline method RITM in terms of mean Intersection over Union (mIoU) on 16 of these datasets. SAM was also evaluated on classical low-level vision tasks such as edge detection, where it produced reasonable edge maps even though it was not explicitly trained for this task. Additionally, its capabilities in generating object proposals and performing instance segmentation were assessed. The model's ability to handle text prompts (though not initially released) was also part of the research evaluation.   

In addition to these broad evaluations, standard performance metrics commonly used in image segmentation were employed to quantify SAM's accuracy. These metrics include Intersection over Union (IoU), which measures the overlap between the predicted segmentation and the ground truth; mean IoU (mIoU), the average IoU over all classes or instances; Precision, which indicates the proportion of true positive pixels among all predicted positive pixels; Recall, the proportion of true positive pixels among all actual positive pixels; Accuracy, the overall correctness of pixel classification; and the F1-Score, which is the harmonic mean of precision and recall. Average Mask Rating, a metric ranging from 1 (nonsense) to 10 (pixel-perfect), was used to assess the subjective quality of the generated masks.   

The stability of SAM's segmentation performance was also scrutinized, particularly in response to variations in the quality of the prompts provided. Metrics for segmentation stability were introduced to evaluate how consistently SAM could segment objects when given imprecise bounding boxes or an insufficient number of points as prompts. These analyses often involved comparisons with methods designed to improve stability, such as Stable-SAM.   

When applied to medical imaging datasets, SAM's zero-shot performance showed significant variability depending on the specific dataset and the segmentation task. For example, the IoU achieved by SAM ranged from a low of 0.1135 for spine MRI to a high of 0.8650 for hip X-ray. This highlighted the challenges that SAM, trained primarily on natural images, faced when applied directly to the medical domain, which often involves images with different characteristics.   

SAM was also thoroughly evaluated on the SA-1B dataset itself to ensure the quality and diversity of the masks that were generated during the data collection process. This evaluation was crucial for validating the integrity of the dataset that underpinned SAM's training and zero-shot capabilities.   

The comprehensive evaluation of SAM across a wide spectrum of tasks and datasets, including comparisons with established state-of-the-art methods and a detailed analysis of its robustness to different types of prompts, provided a clear understanding of the model's strengths and limitations. SAM demonstrated remarkable capabilities in zero-shot generalization and promptable segmentation, proving its versatility across various image domains. However, the evaluations also revealed areas where SAM's performance could vary, such as in specialized medical imaging tasks or when provided with less than ideal prompts, suggesting the need for further research into fine-tuning and domain-specific adaptations. The use of a diverse array of evaluation metrics, encompassing both standard segmentation benchmarks and more specialized measures of mask quality and stability, underscored a multifaceted approach to understanding the true capabilities and reliability of SAM's predictions. Different metrics allowed researchers to capture various aspects of the model's performance, leading to a more complete and nuanced assessment of its strengths and weaknesses.   

3. Segment Anything Model 2 (SAM2): Advancements

3.1 Key Differences and Improvements over SAM

Segment Anything Model 2 (SAM2), released by Meta AI on July 29th, 2024, represents a significant evolution from the original SAM. A primary advancement in SAM2 is its unified model architecture, which is designed to support both image and video segmentation within a single framework. This is a departure from SAM, which primarily focused on still images, although SAM2 treats images as single-frame videos within its architecture. Notably, SAM2 demonstrates improved segmentation accuracy for images compared to its predecessor. Meta AI has reported that SAM2 is approximately six times more accurate than the original SAM in image segmentation tasks. On the SA-23 benchmark, SAM2 achieved a higher accuracy of 58.9% mIoU with a single click compared to SAM's 58.1%. Furthermore, SAM2 offers a significantly faster inference speed for image segmentation, also reported to be around six times quicker than SAM. The model operates at an inference speed of approximately 44 frames per second, enabling real-time applications.   

SAM2 extends the prompt-based object segmentation capabilities of SAM to consistently track objects across video frames in real-time. A key architectural improvement facilitating this is the introduction of a streaming memory architecture, designed for efficient real-time video processing. This architecture processes video frames sequentially and utilizes a memory attention module that allows the model to attend to previous memories of the target object, maintaining context over time. SAM2 also supports dynamic corrections of mask predictions based on supplementary prompts provided by the user throughout the video sequence, allowing for interactive refinement of segmentation results. In the realm of video segmentation, SAM2 has shown superior accuracy, requiring three times fewer user interactions compared to previous models and achieving an eightfold speedup in video annotation tasks. Similar to SAM, SAM2 exhibits strong zero-shot performance, meaning it can effectively segment objects, images, and videos from domains not seen during its training. Meta AI has released four versions of SAM2, varying in size and performance: Tiny (149 MB), Small (176 MB), Base Plus (b+) (309 MB), and Large (856 MB). Generally, the larger models offer greater accuracy but require more computational resources and have slower inference times. The unification of image and video segmentation in SAM2 within a single model represents a significant advancement, addressing the image-centric nature of the original SAM and providing a more versatile foundation model for a broader range of visual tasks. The substantial improvements in both accuracy and speed for image segmentation in SAM2 indicate significant progress in the underlying model architecture and training methodologies.   

3.2 Details on Architectural Improvements

SAM2 incorporates several key architectural enhancements over SAM, primarily focused on enabling video segmentation and improving overall performance.

3.2.1 Streaming Memory: A significant addition to SAM2's architecture is a per-session memory module, which is designed to capture and retain information about the target object throughout a video sequence, facilitating consistent tracking across frames. Unlike SAM, where frame embeddings are directly obtained from an image encoder, the frame embeddings in SAM2's decoder are conditioned on memories of past predictions and frames that have been prompted by the user. This allows the model to leverage historical context and anticipate future object movements. A memory encoder component is responsible for creating and managing these frame memories based on the current predictions made by the model. These memories are then stored in a memory bank, which operates as a First-In, First-Out (FIFO) queue, retaining information about a certain number of recent frames as well as any frames that have been explicitly prompted by the user.   

3.2.2 Memory Attention Module: SAM2 features a memory attention module that plays a crucial role in processing video data. This module takes the per-frame embedding generated by the image encoder and conditions it on the contents of the memory bank. The result is an embedding that has been informed by the historical context of the video, which is then passed on to the mask decoder for generating the segmentation mask. The memory attention module utilizes both self-attention and cross-attention mechanisms to condition the features of the current frame on the features and predictions from previous frames. This integration of past and present information is achieved through a stack of transformer blocks within the module.   

3.2.3 Image and Video Encoder: SAM2 employs a transformer-based architecture known as Hiera (hierarchical image encoder) to extract high-level features from both still images and video frames. For video processing, the image encoder processes the video frames in a sequential manner. A key aspect of SAM2 is that the frame embeddings generated by the image encoder are not independent; instead, they are conditioned on both the predictions made in previous frames and any frames where the user has provided a prompt.   

3.2.4 Prompt Encoder and Mask Decoder: The prompt encoder in SAM2 is similar in design to the prompt encoder used in the original SAM. It is capable of handling various types of prompts, including clicks (both positive to indicate the object and negative to indicate areas to exclude), bounding boxes that roughly outline the object of interest, and masks provided by the user to define the object's initial extent. Sparse prompts like clicks and bounding boxes are represented using positional encodings, which encode their location, combined with learned embeddings that are specific to the type of prompt being used. The mask decoder takes the encoded image features and the encoded prompts as input and generates the final segmentation masks. In the context of video, the mask decoder also utilizes the information from the memory mechanism to ensure accurate tracking of objects across frames. Like SAM, SAM2 is designed to predict multiple masks for a given prompt, allowing it to handle potential ambiguities in the segmentation task. A notable difference from SAM is that SAM2's mask decoder can output the prediction that no valid mask exists in a given frame, which allows it to more robustly handle situations where an object becomes fully occluded.   

3.2.5 Occlusion Prediction: SAM2 introduces a dedicated occlusion prediction head, which is used to assess whether the object of interest is visible in the current frame of a video. To further enhance its ability to handle occlusions and maintain stability over longer video sequences, SAM2 incorporates a no-object embedding in its spatial memories and temporal position encoding in its object pointers. These mechanisms, along with training the model on extended sequences of frames (up to 16 frames in an additional training stage), contribute to improved occlusion handling and long-term tracking stability.   

The introduction of a sophisticated memory mechanism is the primary architectural innovation in SAM2 that underpins its ability to perform video segmentation and to more effectively handle temporal information in visual data. By retaining and utilizing information about past frames and predictions within a memory bank, and through the use of a memory attention module, SAM2 can track objects with greater accuracy and consistency over time. This memory capability also allows the model to better manage challenges such as when an object is temporarily obscured or leaves and re-enters the scene. The conditioning of frame embeddings on both past predictions and user-provided prompts enables SAM2 to effectively leverage the temporal context present in video data, leading to more reliable and accurate segmentation results. This approach allows the model to anticipate how objects might move or change appearance based on their history in the video, and to refine its segmentation predictions accordingly.   

3.3 Training Dataset

SAM2 was trained on a newly created dataset called the Segment Anything Video dataset (SA-V). This dataset is significantly larger than any previously existing video object segmentation dataset, comprising approximately 51,000 videos and over 643,000 segmentation masks, referred to as masklets (object masks over time). The SA-V dataset boasts around 53 times more annotations than the largest prior video segmentation dataset. The process of labeling the videos in SA-V involved a human-in-the-loop approach, where SAM2 itself was used as an interactive tool by human labelers to annotate the video sequences. This annotation process, leveraging the capabilities of SAM2, was found to be approximately 8.4 times faster than having human annotators manually label each frame using the original SAM. The videos included in the SA-V dataset cover a diverse range of real-world scenarios, are geographically representative (collected across 47 countries), and include annotations for whole objects, their parts, as well as instances where objects are occluded or undergo challenging movements. The creation of the SA-V dataset involved a three-phase data engine strategy, starting with using SAM to annotate each frame individually, then combining SAM for spatial masks with an early version of SAM2 for temporal propagation, and finally utilizing the full SAM2 model for interactive annotation. The development of this large-scale and diverse SA-V dataset was crucial for effectively training SAM2's video segmentation capabilities, enabling the model to achieve state-of-the-art performance on established video object segmentation benchmarks such as DAVIS, MOSE, LVOS, and YouTube-VOS. The sheer volume and variety of the training data allowed SAM2 to learn robust spatio-temporal features and to generalize well to new, unseen video data, resulting in significant improvements in segmentation accuracy and a reduced need for user interactions during video analysis.   

4. Medical SAM2: Adaptation for Healthcare

4.1 Key Differences from SAM2

Medical SAM2 (MedSAM-2) represents a significant step in adapting the capabilities of the Segment Anything Model family for the specific challenges of medical image segmentation. Built upon the foundation of SAM2, MedSAM-2 is designed as a generalized auto-tracking model intended for universal application across both 2D and 3D medical image segmentation tasks. A core principle in the development of MedSAM-2 is the approach of treating all 2D and 3D medical segmentation tasks as problems of video object tracking, leveraging the strengths of SAM2 in handling temporal sequences. This philosophy aims to improve segmentation performance in 3D medical images, which can be viewed as a sequence of 2D slices, and to unlock a unique One-Prompt Segmentation capability for 2D medical image flows. With One-Prompt Segmentation, a user can provide a prompt on just one image within a set of 2D medical images, and the model can then autonomously segment the same type of object in all subsequent images, even if there is no temporal relationship between them.   

A key architectural difference in MedSAM-2 is the introduction of a novel self-sorting memory bank mechanism. This mechanism dynamically selects the most informative embeddings based on their confidence and dissimilarity, without being constrained by the temporal order of the images or slices. This is particularly beneficial for 3D medical image segmentation, where spatial continuity between slices is important, and for 2D image sets that may not have a temporal relationship. While some research suggests that SAM2's performance in 2D medical image segmentation might not consistently surpass that of the original SAM, especially in modalities with lower contrast such as CT and ultrasound, MedSAM-2 is designed to address these limitations and has demonstrated superior performance on various medical benchmarks, including MRI which is a higher contrast modality. Furthermore, MedSAM-2 is intended to be a universal model capable of zero-shot generalization to new medical images, meaning it can be applied to visually new content without requiring custom adaptations or fine-tuning for specific tasks.   

The core distinction of MedSAM-2 lies in its innovative adaptation of SAM2's video processing framework to effectively handle the unique characteristics of medical images, including 3D volumes and collections of 2D slices. By conceptualizing medical data as video sequences, MedSAM-2 can leverage SAM2's temporal memory and object tracking capabilities to achieve improved segmentation accuracy and consistency across related medical images, thereby addressing the inherent challenges of generalization and the need to process diverse data formats within the medical domain. The introduction of the self-sorting memory bank in MedSAM-2 is a critical architectural advancement that enables it to efficiently process both unordered 2D medical images and 3D volumes. This goes beyond the temporal order dependence typically found in standard video processing models. Given that medical image datasets often consist of independent slices or volumes that lack a strict temporal relationship, this self-sorting mechanism allows the model to learn from and give higher weight to the most relevant embeddings, regardless of their sequential order, ultimately leading to enhanced segmentation performance in both 2D and 3D medical imaging tasks.

4.2 Fine-tuning Process from SAM2

MedSAM-2 is developed by fine-tuning the pre-trained SAM2 model on a large-scale, carefully curated medical image dataset. This extensive dataset comprises over 455,000 3D image-mask pairs and more than 76,000 annotated video frames, encompassing a wide range of medical imaging modalities such as CT, PET, MRI, ultrasound, and endoscopy. The fine-tuning process often involves adapting specific components of the SAM2 architecture, such as the mask decoder, or introducing lightweight adapter layers into the image encoder to enable the model to better capture the unique features and patterns present in medical images. Some research has also explored fine-tuning only the prompt encoder and the mask decoder while keeping other parts of the model frozen. In certain approaches, the entire model, including the encoder, decoder, memory components, and prompt encoder, is fine-tuned end-to-end to maximize the model's adaptation to the medical domain.   

The datasets used for fine-tuning MedSAM-2 are comprehensive and cover a diverse array of anatomical structures and medical conditions. Examples include the BTCV dataset, used for abdominal multi-organ segmentation; the REFUGE dataset, focusing on optic cup segmentation in fundus images; chest CT datasets for segmenting lungs, heart, and trachea; and various breast imaging datasets. A variety of fine-tuning techniques are employed to adapt SAM2 for medical applications, including the use of adapter modules, ScConv modules, gated attention mechanisms, Low-Rank Adaptation (LoRA), and full model fine-tuning. Parameter-efficient learning strategies, which aim to achieve good performance with a smaller number of trainable parameters, are often investigated to reduce computational costs and the risk of overfitting on the potentially limited medical data. Additionally, some fine-tuning processes incorporate calibration heads during training to ensure that the model learns to associate higher confidence scores with more accurate segmentation predictions, which can enhance the effectiveness of the confidence memory bank used in MedSAM-2.   

The process of fine-tuning SAM2 on large and diverse medical image datasets is crucial for enabling the model to adapt its general image understanding capabilities, learned from natural images, to the specific visual features and characteristics of medical imaging data. Medical images often exhibit different textures, contrasts, and anatomical structures compared to natural images, necessitating a fine-tuning phase where the model can learn domain-specific representations. The significant performance improvements observed after fine-tuning highlight the importance of this adaptation to the medical domain. The variety of fine-tuning techniques that are explored, ranging from parameter-efficient methods like adapter layers and LoRA to more comprehensive full fine-tuning of the model, indicates an ongoing effort within the research community to determine the most effective strategies for adapting large foundation models like SAM2 to the field of medical imaging. These different techniques offer various trade-offs concerning the number of trainable parameters, the time required for training, and the extent to which the model's original pre-trained knowledge is retained versus being adapted to the new domain. Parameter-efficient methods are particularly appealing for very large models as they can reduce the computational resources needed for fine-tuning.

4.3 Impact of Fine-tuning on Performance in Medical Domains

Fine-tuning SAM and SAM2 on medical imaging data has generally been shown to significantly improve performance compared to applying these models in a zero-shot manner. In fact, fine-tuned models like MedSAM-2 have often achieved state-of-the-art results on various medical image segmentation benchmarks, as measured by metrics such as the Dice Score. MedSAM-2 has demonstrated superior performance over other existing models, including those that are specifically tailored for certain tasks and those that are more generally applicable, in segmenting both organs and lesions in different medical imaging modalities like CT, MRI, and PET, even for structures that are traditionally challenging to segment. Furthermore, the use of MedSAM-2 in human-in-the-loop annotation pipelines has led to substantial reductions in the time required for annotating large-scale medical image datasets, indicating its potential for improving efficiency in research and clinical settings. For specific medical segmentation tasks, such as the segmentation of breast tissue without including artifacts like implants or nipple markers, fine-tuning SAM2 using approaches like SAM2-UNet has yielded promising results, showing significant improvements in both accuracy and robustness. While the zero-shot performance of SAM and SAM2 on medical images can be inconsistent and may vary across different imaging modalities, fine-tuning helps to address issues related to the often low contrast and indistinct boundaries found in medical images, resulting in more precise and reliable segmentations. Research has also indicated that fine-tuning SAM on medical imaging data can lead to performance that is better than that achieved by previous segmentation methods that were not based on foundation models. The overall impact of fine-tuning is a significant improvement in the model's ability to accurately and consistently segment a wide range of structures in medical images, making these foundation models more applicable and effective for healthcare-related tasks. The success of MedSAM-2 in outperforming existing models on various benchmarks underscores the effectiveness of combining SAM2's video processing architecture and its self-sorting memory bank with targeted fine-tuning strategies to achieve state-of-the-art results in medical image segmentation across a diverse set of modalities and clinical applications.   

5. Fine-tuning Medical SAM2 for Custom MRI Data

6. Building a Notebook Example for Fine-tuning Medical SAM2

7. Applying Medical SAM2 to an MRI Image

8. Conclusion and Future Directions

The study of the Segment Anything Model (SAM) and its subsequent evolutions, SAM2 and Medical SAM2, reveals a significant paradigm shift in the field of image segmentation. The original SAM introduced the concepts of promptable segmentation and zero-shot transfer, demonstrating impressive capabilities across a wide range of natural images. Its architecture, comprising a heavy image encoder and a lightweight prompt encoder and mask decoder, along with its training on the massive SA-1B dataset, laid the foundation for a new generation of segmentation models.   

SAM2 built upon this foundation by extending its functionality to video segmentation and improving its performance and efficiency in image segmentation tasks. The introduction of a memory mechanism allowed SAM2 to handle temporal information, a crucial step towards processing more complex visual data.   

Medical SAM2 represents a targeted adaptation of SAM2 for the healthcare domain, addressing the unique challenges of medical image segmentation across 2D and 3D data. By treating medical images as video sequences and incorporating innovations like the self-sorting memory bank, MedSAM-2 has shown promising results in various medical imaging modalities, including MRI. Fine-tuning these models on large medical datasets is essential for achieving high accuracy in clinical applications.   

Future research directions could focus on further improving the performance of Medical SAM2 on specific medical image modalities like MRI, particularly in challenging scenarios such as segmenting small or low-contrast structures. Exploring more efficient fine-tuning techniques that require less data and computational resources would also be beneficial for wider adoption in clinical practice. Additionally, investigating the integration of Medical SAM2 with other AI technologies, such as diagnostic tools and treatment planning systems, could unlock new possibilities in healthcare. The development of more robust prompting strategies specifically tailored for medical images, beyond bounding boxes and points, could also enhance the model's usability and accuracy. As the field continues to evolve, the Segment Anything Model family holds significant potential to transform medical image analysis, aiding in more accurate diagnoses, personalized treatment plans, and advancements in medical research.   


Sources used in the report
